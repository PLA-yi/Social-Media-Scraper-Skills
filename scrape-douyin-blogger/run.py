#!/usr/bin/env python3
"""æŠ–éŸ³åšä¸»ä¸»é¡µçˆ¬å– â€” è°ƒç”¨æœ¬åœ° scraper æœåŠ¡ API"""

import argparse
import json
import sys
import urllib.request

SERVER = "http://localhost:8000"


def check_server():
    try:
        urllib.request.urlopen(SERVER, timeout=3)
        return True
    except Exception:
        return False


def main():
    parser = argparse.ArgumentParser(description="æŠ–éŸ³åšä¸»ä¸»é¡µçˆ¬å–")
    parser.add_argument("--url", required=True, help="åšä¸»ä¸»é¡µé“¾æ¥ï¼ˆæ”¯æŒçŸ­é“¾ï¼‰")
    parser.add_argument("--count", type=int, default=20, help="é‡‡é›†è§†é¢‘æ•°é‡")
    args = parser.parse_args()

    if not check_server():
        print("âŒ æœåŠ¡æœªå¯åŠ¨ï¼Œè¯·å…ˆåœ¨é¡¹ç›®ç›®å½•æ‰§è¡Œï¼špython3 server.py")
        sys.exit(1)

    print(f"â–¶ æŠ–éŸ³åšä¸»ä¸»é¡µçˆ¬å–")
    print(f"  é“¾æ¥ï¼š{args.url}  æ•°é‡ï¼š{args.count}")

    payload = json.dumps({
        "platform": "douyin",
        "target": args.url,
        "mode": "blogger",
        "count": args.count,
    }).encode()

    req = urllib.request.Request(
        f"{SERVER}/api/scrape",
        data=payload,
        headers={"Content-Type": "application/json"},
        method="POST",
    )
    with urllib.request.urlopen(req, timeout=10) as resp:
        result = json.loads(resp.read())

    print(f"âœ… ä»»åŠ¡å·²æäº¤ï¼š{result}")
    print(f"ğŸ“‹ å®æ—¶æ—¥å¿—ï¼š{SERVER}")


if __name__ == "__main__":
    main()
